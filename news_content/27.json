{
  "id": "27",
  "title": "用數據做酷的事！手把手教你搭建問答系統",
  "species": "news",
  "content": "＜！default.png！＞／／ｎ我最近很愉快地完成了斯坦福深度學習自然語言處理課程（CS224N），學到了很多新的東西。在結課項目中我基於斯坦福問答數據集（SQuAD）實現了一個問答系統。在這篇博客中，我將為大家介紹搭建問答系統所需要的主要模塊。／／ｎ／／ｎ完整代碼 GitHub 地址：https://github.com/priya-dwivedi/cs224n-Squad-Project／／ｎ／／ｎSQuAD 數據集／／ｎ／／ｎ斯坦福問答數據集（SQuAD）是一個全新的閱讀理解數據集，由眾包人員基於一系列維基百科文章的提問和對應的答案構成，其中每個問題的答案是相關文章中的文本片段或區間。SQuAD包含關於 500 多篇文章的超過 100000 個問答對，規模遠遠超過其他閱讀理解數據集。／／ｎ／／ｎ最近一段時間，各種類型的模型在 SQuAD 數據集上的效果獲得了快速的發展，其中最新的一些模型在問答任務中甚至取得了和人類相當的準確率。／／ｎ／／ｎSQuAD 數據集中的語境、問題和答案的示例／／ｎ／／ｎ語境：阿波羅計劃於／／ｎ1962 至 1972 年間進行，期間得到了同期的雙子座計劃（1962 年 – 1966年）的支持。雙子座計劃為阿波羅計劃成功必需的一些太空旅行技術做了鋪墊。阿波羅計劃使用土星系列火箭作為運載工具來發射飛船。這些火箭還被用於阿波羅應用計劃，包括1973 年到 1974 年間支持了三個載人飛行任務的空間站 Skylab，以及 1975年和前蘇聯合作的聯合地球軌道任務阿波羅聯盟測試計劃。／／ｎ／／ｎ問題：哪一個空間站於 1973 到 1974 年間承載了三項載人飛行任務？／／ｎ／／ｎ答案：Skylab 空間站／／ｎ／／ｎSQuAD 的主要特點：／／ｎ／／ｎi) SQuAD 是一個封閉的數據集，這意味著問題的答案通常位於文章的某一個區間中。／／ｎ／／ｎii) 因此，尋找答案的過程可以簡化為在文中找到與答案相對應部分的起始索引和結束索引。／／ｎ／／ｎiii) 75% 的答案長度小於四個單詞。／／ｎ／／ｎ機器理解模型關鍵組件／／ｎ／／ｎi) 嵌入層／／ｎ／／ｎ該模型的訓練集包括語境以及相關的問題。二者都可以分解成單獨的單詞，這些單詞會被轉換成使用預訓練向量（如GloVe）的詞嵌入。想了解更多關於詞嵌入的信息，參考《教程 | 用數據玩點花樣！如何構建 skim-gram 模型來訓練和可視化詞向量》。同one hot 向量相比，用詞嵌入方式對單詞進行表示可以更好地捕捉語境信息。考慮到沒有足夠的數據，我使用了 100 維的 GloVe詞嵌入並且在訓練過程中沒有對它們進行修改。／／ｎ／／ｎi) 編碼器層／／ｎ＜！https://www.xcnnews.com/wp-content/uploads/2018/04/20180405_5ac63899c5b14.jpg！＞／／ｎRNN 編碼器／／ｎ／／ｎ我們將基於RNN 的編碼器加入到了模型的下一層當中。我們希望語境中的每一個單詞能和它前後的單詞產生聯繫。雙向 GRU/LSTM可以幫助我們達到這一目標。RNN 的輸出是一系列向前、向後的隱藏向量，然後我們會將它們級聯起來。類似地，我們可以使用相同的 RNN編碼器創建問題隱藏向量。／／ｎ／／ｎiii）注意力層／／ｎ／／ｎ現在我們有了一個語境隱藏向量和問題隱藏向量。我們需要將這兩個向量結合起來，以找到問題的答案。這時就需要用到注意力層。注意力層是問答系統的關鍵組成部分，因為它能幫助確定對於給定的問題我們應該「注意」文中的哪些單詞。讓我們從最簡單的注意力模型開始：／／ｎ／／ｎ點積注意力／／ｎ＜！https://www.xcnnews.com/wp-content/uploads/2018/04/20180405_5ac6389a9acd7.jpg！＞／／ｎCS224N 中基本注意力的可視化分析／／ｎ／／ｎ點積注意力等於每個語境向量c_i 乘每個問題向量 q_j 的結果向量 e^i（上圖中的注意力分數）。之後，我們對 e^i 調用 softmax 函數來得到α^i（上圖中的注意力分佈）。softmax 保證了所有 e^i 的和是 1。最終，我們計算出 a_i：注意力分佈 α^i與對應問題向量（上圖中的注意力輸出）的積。點積注意力也可以用下面的式子來描述：／／ｎ＜！https://www.xcnnews.com/wp-content/uploads/2018/04/20180405_5ac6389b225a1.jpg！＞／／ｎ上面提到的注意力已作為基線注意力機制在 GitHub 代碼中實現。／／ｎ／／ｎ更複雜的注意力——BiDAF 注意力／／ｎ／／ｎ你可以用上述基本注意力層來運行 SQuAD 模型，但恐怕結果不盡人意。更複雜的注意力才能產出更好的性能。／／ｎ／／ｎ我們來了解一下 BiDAF 論文（https://arxiv.org/abs/1611.01603）。該論文的主要觀點是注意力應該是雙向的——從語境到問題和從問題到語境。／／ｎ／／ｎ我們首先計算相似度矩陣 S ∈ R^N×M，它包含每對語境和問題隱藏狀態 (c_i , q_j) 的相似度分數。這裡／／ｎ＜！https://www.xcnnews.com/wp-content/uploads/2018/04/20180405_5ac6389b6fae6.jpg！＞／／ｎc_i ◦ q_j 代表數組元素對應相乘，w_sim ∈ R 6h 是權重向量。S_ij 用下面的式子來表述：／／ｎ＜！https://www.xcnnews.com/wp-content/uploads/2018/04/20180405_5ac6389b809b0.jpg！＞／／ｎ之後，我們將展示 C2Q 注意力（與上面提到的點積注意力類似）。我們對 S 逐行調用 softmax 函數來獲得注意力分佈 α^i，用它得到問題隱藏狀態 q_j 的加權和，最後得出 C2Q 注意力的輸出 a_i。／／ｎ／／ｎ＜！https://www.xcnnews.com/wp-content/uploads/2018/04/20180405_5ac6389bec400.jpg！＞／／ｎ現在，我們來執行 Q2C 注意力。對於每一個語境位置 i ∈ {1, . . . , N}，我們取相似度矩陣對應行的最大值：／／ｎ＜！https://www.xcnnews.com/wp-content/uploads/2018/04/20180405_5ac6389c51b8d.jpg！＞／／ｎ之後我們對結果向量 m ∈ R^N 調用 softmax 函數，而這將給出關於語境位置的注意力分佈 β ∈ R^N。之後，我們使用 β 得到語境隱藏狀態的加權和 c_i，這也是 Q2C 注意力的輸出結果 c’。以下是相關公式：／／ｎ＜！https://www.xcnnews.com/wp-content/uploads/2018/04/20180405_5ac6389cbe590.jpg！＞／／ｎ最終對於每一個語境位置 c_i，我們結合 C2Q 注意力和 Q2C 注意力的輸出，下面是相關公式：／／ｎ／／ｎ如果你覺得這一段令人費解，不用擔心，注意力確實是一個複雜的話題。你可以試著一邊喝茶，一邊閱讀這篇 BiDAF 論文。／／ｎ／／ｎiv) 輸出層／／ｎ／／ｎ我們就快成功了。模型的最後一層是一個softmax輸出層，它幫助我們找出答案區間的開始和結束索引。我們通過結合語境隱藏狀態和之前層的注意力向量來得到混合的結果。這些混合的結果最終會成為全連接層的輸入，該層使用softmax 來得到 p_start 向量（具備開始索引的概率）以及 p_end結束（具備結束索引的概率）。我們知道大部分答案從開始索引到結束索引最多 15 個單詞，由此我們可以尋找使 p_start 與 p_end乘積最大的開始和結束索引。／／ｎ／／ｎ損失函數是開始和結束位置的交叉熵損失之和。它使用 Adam Optimizer 來獲得最小值。／／ｎ／／ｎ我構建的最終模型比上面描述的要複雜一點，在利用測試集測試時獲得了 75 分的 F1 分數。還行！／／ｎ／／ｎ下一步／／ｎ／／ｎ關於未來探索的一些想法：／／ｎ／／ｎ由於 CNN 運行起來比 RNN 快得多，並且更容易在 GPU 上并行計算，因此我最近一直都在用基於 CNN 的編碼器而非上述 RNN 編碼器進行實驗。／／ｎ／／ｎ其他的注意力機制，如 Dynamic Co-attention（https://arxiv.org/abs/1611.01604）／／ｎ／／ｎ原出處:中國新聞,https://www.xcnnews.com/kj/3675772.html／／ｎ＜％https://www.xcnnews.com/kj/3675772.html％＞",
  "user": "中國新聞",
  "news_date": "2018-02-09",
  "create_time": "2018-02-13",
  "vanish_time": "2019-02-15"
}